#+title: Notes

* Tasks
** DONE basic configuration traits
** DONE optional feature to enable http requests
** TODO implement default debug trait for Configurable
** need workflows
- [ ] coverage
- [X] tests/clippy/check/fmt
- [ ] integration tests (require only redis now)
** TODO healthcheck
- [X] internet
- [ ] proxy with httpbin
- [ ] optional redis
- [ ] optional mongodb
- [ ] optional postgres
** DONE generic executor
- [X] basic version
- [X] tests for basic version
- [X] example for basic version
- [X] make in example in `examples/` folder
- [X] remove E from Task type, use string representation instead
- [X] in-memory backend for Tasks storage
- [X] split storage trait into separate module
- [X] docs for strage
- [X] docs for tasks
- [X] docs for executor
- [X] add workers settings
- [X] builder pattern for executor settings
** TODO further adjustments
- [X] re-export common crate - thiserror
- [X] pass context where db connections could be stored
- [X] rename example/basic.rs into executor_basic.rs
- [X] make_sorage return TaskStorage instead of wrapped with Arc<>
- [X] fix max_retries
- [X] fix tests after adding context to the executor
- [ ] adjust async tests to [tokio::test] and drop initialization of async runtime in code
- [X] task fail into storage, to collect totaly (by max_retries) failed tasks
- [X] reexport as much as possible from task_deport
- [X] optional redis storage backend using rustis
- [-] separate tests for redis backend
- [-] try to move tests init common function into separate file in tests/ folder
- [X] use same builder pattern for Application
- [X] pass task_id into the process
- [X] re-export Uuid
- [X] optional dependencies for task_deport
- [X] seriously figure out dependencies and reexport crates
** Refactor worker. Add basic monitoring capabilities.
- [X] worker is struct
- [X] add stats fields to collect worker stats
- [X] rename executor/executor.rs to something else
- [X] move WorkerStats into the separate module
- [ ] expose http endpoint with json collected basic stats from workers
** Refactor processor/task_runner.
- [ ] shared context for processor. way to transform &Worker to SharedContext
- [X] make example work with improved processor
- [ ] make better naming for processor
- [ ] add storage error types context error types to TaskProcessorError
** Refactor storage (task deport)
- [X] new type for TaskId
- [X] remove custom error type for simplicity (no SE generic anymore yay!)
** Executor
- [ ] figure out how to move constrains from direct name of TaskStorage to something like Box<dyn impl TaskStorage>
- [ ] better task handling in executor, queue, started, finished and TaskStatus enum
- [ ] need catch unwind to handle all possible exceptions
- [ ] move to another workspace
** Task Deport
- [ ] add queued time, make started time optional (should be something when execution is started)
- [ ] split TaskStorage and TaskQueue implementation where task storage is pluggable
- [ ] drop task exceed time for execution
- [ ] separate thread to clean up staled tasks (or find the way how to not make them stalled)
- [ ] add more tests
** Important!!!!
- [-] TaskProcessor::process should not panic? (ugh it's complicated to apply catch_unwind to async function)
- [X] thiserror should not be optional
- [ ] Use tokio per core executor to make tasks Sync maybe? if possible
- [X] implement purge for TaskStorage trait
- [ ] make whole capp-rs as async tasks executor for web crawling purposes
- [ ] i think need to remove unnecessary workspaces

** redis storage backend with bloom filter
** redis storage backend with priorities

* Notes
** httpbin could be launched as container "docker run -p 80:80 kennethreitz/httpbin"

* GPT area

Hello fancy AI, I'm struggle to implement one feature for my crate (Rust language).
Let's say i have trait:

/// A trait defining the interface for processing a task. This trait is
/// intended to be implemented by a worker that will process tasks
/// of a specific type.
#[async_trait]
pub trait Computation<Data, Store, Ctx>
where
    Data: Clone + Serialize + DeserializeOwned + Send + Sync + 'static,
    Store: TaskStorage<Data> + Send + Sync + 'static,
    Ctx: Send + Sync + 'static,
{
    /// Processes the task. The worker_id is passed for logging or
    /// debugging purposes. The task is a mutable reference,
    /// allowing the processor to modify the task data as part of the processing.
    async fn run(
        &self,
        worker_id: WorkerId,
        ctx: Arc<Ctx>,
        storage: Arc<Store>,
        task: &mut Task<Data>,
    ) -> Result<(), ComputationError>;
}

Which run in such context:

    let ctx = Arc::new(Context::from_config(config_path));
    let storage = InMemoryTaskStorage::new();
    let computation = Arc::new(MyComputation {});
    let executor_options = ExecutorOptionsBuilder::default()
        .task_limit(30)
        .concurrency_limit(2_usize)
        .build()
        .unwrap();
    capp::run_workers(ctx, computation, storage, executor_options).await;


And "run_workers" defined as:

pub async fn run_workers<D, P, S, C>(
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};    ctx: Arc<C>,
    processor: Arc<P>,
    storage: Arc<S>,
    options: ExecutorOptions,
) where
    D: Clone
        + Serialize
        + DeserializeOwned
        + Send
        + Sync
        + 'static
        + std::fmt::Debug,
    P: Computation<D, S, C> + Send + Sync + 'static,
    S: TaskStorage<D> + Send + Sync + 'static,
    C: Configurable + Send + Sync + 'static,
{
    let (shutdown_tx, shutdown_rx) = tokio::sync::watch::channel(());
    let limit_notify = Arc::new(tokio::sync::Notify::new());
    let task_counter = Arc::new(AtomicU32::new(0));

    let mut worker_handlers = Vec::new();

    for i in 1..=options.concurrency_limit {
        worker_handlers.push(tokio::spawn(worker_wrapper::<D, P, S, C>(
            WorkerId::new(i),
            Arc::clone(&ctx),
            Arc::clone(&storage),
            Arc::clone(&processor),
            Arc::clone(&task_counter),
            options.task_limit,
            Arc::clone(&limit_notify),
            shutdown_rx.clone(),
            options.worker_options.clone(),
        )));
    }

    tokio::select! {
        _ = tokio::signal::ctrl_c() => {
            tracing::warn!("Ctrl+C received, shutting down...");
            shutdown_tx.send(()).unwrap();
        }
        _ = limit_notify.notified() => {
            tracing::warn!("Task limit reached, shutting down...");
            shutdown_tx.send(()).unwrap();
        }
    }

    let results = futures::future::join_all(worker_handlers).await;
    for result in results {
        if let Err(e) = result {
            tracing::error!("Fatal error in one of the workers: {:?}", e);
        }
    }
}
