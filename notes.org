#+title: Notes

* Tasks
** DONE basic configuration traits
** DONE optional feature to enable http requests
** TODO implement default debug trait for Configurable
** need workflows
- [ ] coverage
- [X] tests/clippy/check/fmt
- [ ] integration tests (require only redis now)
** TODO healthcheck
- [X] internet
- [ ] proxy with httpbin
- [ ] optional redis
- [ ] optional mongodb
- [ ] optional postgres
** DONE generic executor
- [X] basic version
- [X] tests for basic version
- [X] example for basic version
- [X] make in example in `examples/` folder
- [X] remove E from Task type, use string representation instead
- [X] in-memory backend for Tasks storage
- [X] split storage trait into separate module
- [X] docs for strage
- [X] docs for tasks
- [X] docs for executor
- [X] add workers settings
- [X] builder pattern for executor settings
** TODO further adjustments
- [X] re-export common crate - thiserror
- [X] pass context where db connections could be stored
- [X] rename example/basic.rs into executor_basic.rs
- [X] make_sorage return TaskStorage instead of wrapped with Arc<>
- [X] fix max_retries
- [X] fix tests after adding context to the executor
- [ ] adjust async tests to [tokio::test] and drop initialization of async runtime in code
- [-] task fail into storage, to collect totaly (by max_retries) failed tasks
- [ ] reexport as much as possible from task_deport
- [X] optional redis storage backend using rustis
- [-] separate tests for redis backend
- [-] try to move tests init common function into separate file in tests/ folder
- [X] use same builder pattern for Application
- [X] pass task_id into the process
- [X] re-export Uuid
- [X] optional dependencies for task_deport
- [X] seriously figure out dependencies and reexport crates
** Make Worker a struct
** Executor
- [ ] figure out how to move constrains from direct name of TaskStorage to something like Box<dyn impl TaskStorage>
- [ ] better task handling in executor, queue, started, finished and TaskStatus enum
- [ ] need catch unwind to handle all possible exceptions

** Task Deport
- [X] move Task<D> into the separate module
- [X] move backends into backends/
- [X] rename Task.data into Task.payload and add function get_payload()
- [ ] add queued time, make started time optional (should be something when execution is started)
- [X] move executor module to new style (w/o mod.rs)
- [ ] split TaskStorage and TaskQueue implementation where task storage is pluggable
- [ ] add task status which is enum
- [ ] drop task exceed time for execution
- [ ] thread to clean up staled tasks
- [ ] dead latter queue
- [ ] add more tests

** redis storage with bloom filter

* Notes
** httpbin could be launched as container "docker run -p 80:80 kennethreitz/httpbin"
** That's the plan:
///  1. **Task Creation:**
/// When a new task is created, it gets a unique `task_id` (this could be
/// a UUID or some other unique value). The task is serialized and stored
/// in a hashmap with the `task_id` as the key. The `task_id` is also
/// pushed to a list which acts as a task queue.
///
/// 2. **Task Execution:**
/// Workers pop `task_id` values from the task queue (the list data structure).
/// They then retrieve the corresponding serialized Task from the
/// hashmap using the `task_id`, deserialize the Task and start executing it.
///
/// 3. **Task Status Update:**
/// As a Task is being processed, its status is updated (started, finished,
/// retries, failed_with, etc.) and the updated Task is serialized and stored back
/// in the Redis hashmap. This allows the status of the Task to be tracked
/// in real-time.
///
/// 4. **Task Completion:**
/// When a Task is completed, its final status is updated in the hashmap.
/// If the task needs to be removed from storage after completion, it can be
/// deleted from the hashmap.
///
/// 5. **Task Failure and Retry:**
/// If a task fails, its `failed_with` field is set with the error,
/// `retries` is incremented, and it's pushed back into the storage list
/// for reprocessing by a worker.

** Previous plan turned into shit, i need another one.
The main issue is how to pass arbitrary context from top to the bottom.
Bottom parts do not need huge blob of unrelated data in context.

Also i need to pass connections to the databases, which could have differnet
constrains.


